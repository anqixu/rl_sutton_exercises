{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.1: Self-Play\n",
    "* If the agent's policy makes near-random actions, resembling that of a random opponent, then self-play would naturally result in similar policy improvements as if playing against a random opponent\n",
    "* On the other hand, if the agent's policy is highly biased to favor a specific part of state-action space, and it has little exploratory attributes, then self-play may result in biased experiences that optimize moves only for those parts of the state-action space and not elsewhere\n",
    "* This type of biased self-play may still be advantageous if the agent's policy has sufficient exploratory attributes and/or is exploring in parts of the state-space with high task-optimality, as shown by AlphaGo and AlphaGo Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.2: Symmetries\n",
    "* Tic-tac-toe board has positional symmetrics across the center vertical column, center vertical row, both diagonals, and is also symmetric at $90^\\circ$ rotations\n",
    "  * More programmatically, there are 8 symmetric configurations given a $3x3$ board $B$: $\\left\\{B, rot\\left(B, 90^\\circ\\right), rot\\left(B, 180^\\circ\\right), rot\\left(B, 270^\\circ\\right), B^T, rot\\left(B^T, 90^\\circ\\right), rot\\left(B^T, 180^\\circ\\right), rot\\left(B^T, 270^\\circ\\right)\\right\\}$\n",
    "* One way to exploit symmetry is to transform the state of each episode to a standard configuration $B$\n",
    "  * Concretely, the transformation is determined by rotating(90 | 180 | 270) and/or transposing the board, so that the *first move of player A* is on *either the top-left or top-middle square*, and the *first move of player B* is within *the upper-triangular region* of the board\n",
    "  * Consequently, this would aggregate symmetric sequences of state-action-state-reward transitions together, thus facilitating faster policy learning\n",
    "* If the opponent plays weaker moves in a specific subset of board configurations, but plays stronger in a symmetric variant of the board, then our agent can exploit this assymmetry by being non-symmetric also\n",
    "* Based on the above argument, we conclude that symmetrically equivalent positions should *not* necessarily have the same value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.3: Greedy Play\n",
    "* In general, a greedy agent might miss out on a multi-step strategy that makes short-term sacrifices in order to ultimately gain better long-term reward\n",
    "* For tic-tac-toe though, once a player makes a threatening move (by connecting 2 out of 3 cells in a line), then the other player is almost always forced to play defensively, and can at most result in a draw; thus it's not apparent whether being greedy will drastically change the outcome of this game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.4: Learning from Exploration\n",
    "* By decaying the step-size parameter $\\alpha$ for the value function updates $V(s)$, these estimates will eventually converge to *some* value, even though this value may or may not have a practical relationship with the true value function\n",
    "* However, if we employ a greedy policy based on these estimates, then the ones that include exploratory moves will generally result in lower value functions, as they would include (low) rewards obtained through epsilon-random exploration\n",
    "* Assume that we use the same epsilon-exploration selection strategy to augment these greedy policies; in cases where epsilon-exploration is selected, both policies would behave identically; otherwise the policy that excludes epsilon-explored reward estimates would have *fewer* estimates but *more optimal* estimates (i.e. **high-bias estimates**), while the one that included epsilon-explored rewards would have *more* estimates but *noisier* estimates (i.e. **biased and high-variance estimates**)\n",
    "* After convergence, by definition the value functions would cease to change (due to a zero-valued step-size), so both policies would be equally **incapable** of learning more\n",
    "* Personally, I suspect that the policy that did not include epsilon-explored rewards would result in more wins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.5: Other Improvements\n",
    "* As discussed in Exercise 1.3, once one of the players start making a threatening move, in most cases the rest of the moves becomes deterministic if the defending player does not want to immediately lose, and the attacking player does not want to give up an advantageous board position\n",
    "* In these late-game states, there is no need for the reinforcement learning agent to make exploratory moves, once we can exhaustively search through the action space and determine that there is truly only one best-case move at each time instant\n",
    "* Extending the above, since the game of tic-tac-toe has a small state space ($2^9$ states and $9$ actions), we can employ a Game Theoretic search tree approach to exhaustively search and determine optimal moves in every possible game state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
