{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.1: Self-Play\n",
    "* If the agent's policy makes near-random actions, resembling that of a random opponent, then self-play would result in similar policy improvements as if playing against a random opponent\n",
    "* If the agent's policy is highly biased to favor a specific part of state-action space, and it has little explorative attributes, then self-play may result in optimizing moves for that part of state-action space yet not perfecting for other parts\n",
    "* This type of biased self-play may be advantageous, if the agent's policy has sufficient exploratory attributes, as seen from AlphaGo and AlphaGo Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.2: Symmetries\n",
    "* Tic-tac-toe board has positional symmetrics across the center vertical column, center vertical row, both diagonals, and is also symmetric at $90^\\circ$ rotations\n",
    "* One way to exploit symmetry is to rotate and/or reflect the board so that the *first move of player A* is on *either the top-left or top-middle square*, and the *first move of player B* is within *the upper-triangular region* of the board; this would disambiguate symmetric starting moves so that the agent can learn a policy faster, since the first few moves in stat-action space would be more concentrated\n",
    "* If the opponent plays weaker moves in a specific subset of board configurations, but plays stronger in a symmetric variant of the board, then our agent can exploit this assymmetry by being non-symmetric also\n",
    "* Based on the above argument, we conclude that symmetrically equivalent positions should *not* necessarily have the same value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.3: Greedy Play\n",
    "* In general, a greedy agent might miss out on a multi-step strategy that makes short-term sacrifices in order to ultimately gain better long-term reward\n",
    "* For tic-tac-toe though, once a player makes a threatening move (by connecting 2 out of 3 cells in a line), then the other player is almost always forced to play defensively, and can at most result in a draw; thus it's not apparent whether being greedy will drastically change the outcome of this game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.4: Learning from Exploration\n",
    "* By decaying the step-size parameter $\\alpha$ for the value function updates $V(s)$, these estimates will eventually converge to some point\n",
    "* However, if we employ a greedy policy based on these estimates, then the ones that include exploratory moves will generally result in lower value functions, as they would include (low) rewards obtained through epsilon-random exploration\n",
    "* Assume that we use the same epsilon-exploration selection strategy to augment these greedy policies; in cases where epsilon-exploration is selected, both policies would behave identically; otherwise the policy that did not include epsilon-explored rewards would have *fewer* estimates but *more advantageous* estimates (i.e. **high-bias estimates**), while the one that included epsilon-explored rewards would have *more* estimates but *noisier* estimates (i.e. **biased and high-variance estimates**)\n",
    "* After convergence, by definition the value functions would cease to change (due to a zero-valued step-size), so both policies would be equally incapable of learning more\n",
    "* Personally, I suspect that the policy that did not include epsilon-explored rewards would result in more wins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.5: Other Improvements\n",
    "* As discussed in Exercise 1.3, once one of the players start making a threatening move, in most cases the rest of the moves becomes deterministic if the defending player does not want to immediately lose, and the attacking player does not want to give up an advantageous board position\n",
    "* In these late-game states, there is no need for the reinforcement learning agent to make exploratory moves, once we can exhaustively search through the action space and determine that there is truly only one best-case move at each time instant\n",
    "* Extending the above, since the game of tic-tac-toe has a small state space ($2^9$ states and $9$ actions), we can employ a Game Theoretic search tree approach to exhaustively search and determine optimal moves in every possible game state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
