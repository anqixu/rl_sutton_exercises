{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.1\n",
    "* $Prob(a=a_{greedy})$\n",
    "\n",
    "  $= \\Sigma_{choice \\in \\{random, greedy\\}} Prob(a=a_{greedy}, choice)$\n",
    "  \n",
    "  $= \\Sigma_{choice \\in \\{random, greedy\\}} Prob(a=a_{greedy}~|~choice) * Prob(choice)$\n",
    "  \n",
    "  $= Prob(a=a_{greedy}~|~choice=greedy) \\cdot Prob(choice=greedy) + Prob(a=a_{greedy}~|~choice=random) \\cdot Prob(choice=random)$\n",
    "  \n",
    "  $= 1.0 \\cdot (1.0 - \\epsilon) + 0.5 \\cdot \\epsilon$\n",
    "  \n",
    "  $=0.75$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.2: Bandit example\n",
    "* Initially, $Q_1(A) = [0, 0, 0, 0]$\n",
    "* Thus, $A_1=1$ *may* have been chosen either via greedy or random, since $A_1^{greedy}= 1~|~2~|~3~|~4$\n",
    "\n",
    "* After step 1, $Q_2(A) = [\\frac{1}{1}, 0, 0, 0]$\n",
    "* Thus, $A_2=2$ **must** have been chosen via random, since $A_2^{greedy}=1$\n",
    "\n",
    "* After step 2, $Q_3(A) = [\\frac{1}{1}, \\frac{1}{1}, 0, 0]$\n",
    "* Thus, $A_3=2$ *may* have been chosen via greedy or random, since $A_3^{greedy}=1~|~2$\n",
    "\n",
    "* After step 3, $Q_4(A) = [\\frac{1}{1}, \\frac{1+2}{2}, 0, 0]$\n",
    "* Thus, $A_4=2$ *may* have been chosen via greedy or random, since $A_4^{greedy}=2$\n",
    "\n",
    "* After step 4, $Q_5(A) = [\\frac{1}{1}, \\frac{3+2}{3}, 0, 0]$\n",
    "* Thus, $A_5=3$ **must** have been chosen via random, since $A_5^{greedy}=2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.3\n",
    "* From the top plot in Figure 2.2, we see that $E[Q]^{greedy}=1$\n",
    "* In Sec. 2.3, we are told that $E[Q]^{optimal}=1.55$ for this stationary 10-armed bandit environment\n",
    "* Assuming that the sample-averages for $Q_i$ have converged, then the average performance of $\\epsilon$-greedy action-value methods is $E[Q] = E[Q]^{optimal} \\cdot (1.0-\\epsilon) + E[Q]^{greedy} \\cdot \\epsilon$\n",
    "* Thus, for $\\epsilon=0.1$, $E[Q] = 1.55 \\cdot 0.9 + 1 \\cdot 0.1 = 1.495$; and for $\\epsilon=0.01$, $E[Q] = 1.55 \\cdot 0.99 + 1 \\cdot 0.01 = 1.5445$\n",
    "* Based on these theoretical analyses, and looking at the top plot in Figure 2.2, we see that the $\\epsilon=0.01$ strategy converges slower yet will ultimately converge to a better average reward, whilst the $\\epsilon=0.1$ strategy converges faster yet will ultimately converge to a slightly worse average reward\n",
    "* Both policies noticeably have higher average reward compared to the purely-greedy ($\\epsilon=0$) strategy\n",
    "* Applying the answer to Exercise 2.1, assuming that both $\\epsilon$-greedy policies converge, then each $\\epsilon$-greedy strategy would select the optimal action with a probability of $1.0 \\cdot (1.0-\\epsilon) + 0.1 \\cdot \\epsilon$\n",
    "* Thus, we expect the the $\\epsilon=0.01$ strategy to ultimately converge to a higher optimal action percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.4\n",
    "* If the step-size parameter $\\alpha$ is not constant as a function of the timestep $n$, then we simply need to substitute the correct $\\alpha_n$ value in the unrolled expression in equation (2.6)\n",
    "* For example, assume that we change the step-size once at timestep $n=i$, i.e. $\\alpha_1=\\alpha_2=...=\\alpha_j$ and $\\alpha_{n}=\\alpha_{n-1}=...=\\alpha_{j+1}$, then:\n",
    "\n",
    "  $Q_{n+1}$\n",
    "  \n",
    "  $= Q_n + \\alpha_n \\left[ R_n - Q_n \\right]$\n",
    "  \n",
    "  $= \\alpha_n R_n + (1-\\alpha_n) Q_n$\n",
    "  \n",
    "  $=~...$\n",
    "  \n",
    "  $= \\alpha_n R_n + (1-\\alpha_n) \\alpha_n R_{n-1} +~...$\n",
    "  \n",
    "  $~~+ (1-\\alpha_n)^{n-(j+1)} \\alpha_n R_{j+1} + (1-\\alpha_n)^{n-j} \\alpha_1 R_{j} + (1-\\alpha_n)^{n-j} (1-\\alpha_1) \\alpha_1 R_{j-1} +~...$\n",
    "\n",
    "  $~~+ (1-\\alpha_n)^{n-j} (1-\\alpha_1)^{j-1} \\alpha_1 R_{1} + (1-\\alpha_n)^{n-j} (1-\\alpha_1)^j Q_1$\n",
    "  \n",
    "  $= (1-\\alpha_n)^{n-j} (1-\\alpha_1)^j Q_1 + \\sum_{i=1}^{j} \\alpha_1 (1-\\alpha_1)^{j-i} (1-\\alpha_n)^{n-j} R_i + \\sum_{i=j+1}^{n} \\alpha_n (1-\\alpha_n)^{n-i} R_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.5\n",
    "[see separate notebook](Exercise_2.5.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.6: Mysterious Spikes\n",
    "* The most significant spike in Figure 2.3 occurs at the $(k+1)=11$-th step for the optimistic greedy policy in the $k=10$-armed bandit task\n",
    "* Since all value estimates were set to much higher than their rewards, i.e. $Q_1(a)=5 \\gg R(a)$, the first 10 greedy actions will sample all 10 arms in some arbitrary order, since each time an arm is chosen, its $Q_i(a)$ value would be reduced below the other initial estimates\n",
    "* After 10 steps, the value estimates will be exactly $Q_{11}(a) = Q_1(a) - R(a)$, and so the true optimal arm will have the highest Q value, notwithstanding sample randomness in the reward function\n",
    "* Since there is some sample randomness in the reward function, thus on step 11 the optimal action will be chosen with *high likelihood*, albeit not $100\\%$\n",
    "* The other less-noticeable spikes are similarly attributed to the fact that the optimal arm being selected on the $n k+i$-th timestep, for integer multiple $n=\\{1, 2, ... \\}$, and $i=\\{1, 2, ... 9\\}$, which is due to $Q_{n k + i}(a)- (n-1) R(a) \\gg R(a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.7\n",
    "* The sigmoid function $S(x)$ is defined as:\n",
    "\n",
    "  $S(x) = \\frac{1}{1+e^{-x}}$\n",
    "  \n",
    "* If $k=2$, then let $A_t=\\{a, a'\\}$, and:\n",
    "\n",
    "  $Pr\\left\\{ A_t=a \\right\\} = \\frac{ e^{H_t(a)} }{ \\sum_{b=1}^{k=2} e^{H_t(b)} } = \\frac{ e^{H_t(a)} }{ e^{H_t(a)} + e^{H_t(a')} } = \\frac{ 1 }{ 1 + e^{- \\left( H_t(a) - H_t(a') \\right) } } = S\\left(x=\\left( H_t(a) - H_t(a') \\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.8\n",
    "* Assume that we are using a stochastic policy $\\pi(a=1)=\\alpha$ and $\\pi(a=2)=1-\\alpha$ for $\\alpha \\in [0, 1]$\n",
    "* If we don't know whether each state is drawn from case A or case B, then the expected reward of this policy is:\n",
    "\n",
    "  $E[R] = Pr\\{case=A\\} \\cdot [ R_{a=1}^{case A} \\pi(a=1) + R_{a=2}^{case A} \\pi(a=2) ] + Pr\\{case=B\\} \\cdot [ R_{a=1}^{case B} \\pi(a=1) + R_{a=2}^{case B} \\pi(a=2) ] = 0.5 \\cdot [0.1 \\alpha + 0.2 (1 - \\alpha)] + 0.5 \\cdot [0.9 \\alpha + 0.8 (1 - \\alpha)] = 0.5$\n",
    "  \n",
    "* If we can observe whether each state instance is drawn from case A or case B, then we can keep separate Q estimates for each case. As these estimates converge, a well-tuned policy (e.g. $\\epsilon$-greedy policy with small $\\epsilon$ value) would likely be able to consistently exploit the best action in each case (i.e. action 2 in case A and action 1 in case B)\n",
    "* Thus, the expected reward of such a policy with associative state context sould be:\n",
    "\n",
    "  $E[R] \\approx Pr\\{case=A\\} \\cdot [ argmax_a \\left( R_{a}^{case A} \\right) ] + Pr\\{case=B\\} \\cdot [ argmax_a \\left( R_{a}^{case B} \\right) ] = 0.5 \\cdot (0.2) + 0.5 \\cdot 0.9 = 0.55$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.9\n",
    "[see separate notebook](Exercise_2.9.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
