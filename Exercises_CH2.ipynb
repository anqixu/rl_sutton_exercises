{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.1\n",
    "* $Prob(a=a_{greedy})$\n",
    "\n",
    "  $= \\Sigma_{choice \\in \\{random, greedy\\}} Prob(a=a_{greedy}, choice)$\n",
    "  \n",
    "  $= \\Sigma_{choice \\in \\{random, greedy\\}} Prob(a=a_{greedy}~|~choice) * Prob(choice)$\n",
    "  \n",
    "  $= Prob(a=a_{greedy}~|~choice=greedy) \\cdot Prob(choice=greedy) + Prob(a=a_{greedy}~|~choice=random) \\cdot Prob(choice=random)$\n",
    "  \n",
    "  $= 1.0 \\cdot (1.0 - \\epsilon) + 0.5 \\cdot \\epsilon$\n",
    "  \n",
    "  $=0.75$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.2: Bandit example\n",
    "* Initially, $Q_1(A) = [0, 0, 0, 0]$\n",
    "* Thus, $A_1=1$ *may* have been chosen either via greedy or random, since $A_1^{greedy}= 1~|~2~|~3~|~4$\n",
    "\n",
    "* After step 1, $Q_2(A) = [\\frac{1}{1}, 0, 0, 0]$\n",
    "* Thus, $A_2=2$ **must** have been chosen via random, since $A_2^{greedy}=1$\n",
    "\n",
    "* After step 2, $Q_3(A) = [\\frac{1}{1}, \\frac{1}{1}, 0, 0]$\n",
    "* Thus, $A_3=2$ *may* have been chosen via greedy or random, since $A_3^{greedy}=1~|~2$\n",
    "\n",
    "* After step 3, $Q_4(A) = [\\frac{1}{1}, \\frac{1+2}{2}, 0, 0]$\n",
    "* Thus, $A_4=2$ *may* have been chosen via greedy or random, since $A_4^{greedy}=2$\n",
    "\n",
    "* After step 4, $Q_5(A) = [\\frac{1}{1}, \\frac{3+2}{3}, 0, 0]$\n",
    "* Thus, $A_5=3$ **must** have been chosen via random, since $A_5^{greedy}=2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.3\n",
    "* From the top plot in Figure 2.2, we see that $E[Q]^{greedy}=1$\n",
    "* In Sec. 2.3, we are told that $E[Q]^{optimal}=1.55$ for this stationary 10-armed bandit environment\n",
    "* Assuming that the sample-averages for $Q_i$ have converged, then the average performance of $\\epsilon$-greedy action-value methods is $E[Q] = E[Q]^{optimal} \\cdot (1.0-\\epsilon) + E[Q]^{greedy} \\cdot \\epsilon$\n",
    "* Thus, for $\\epsilon=0.1$, $E[Q] = 1.55 \\cdot 0.9 + 1 \\cdot 0.1 = 1.495$; and for $\\epsilon=0.01$, $E[Q] = 1.55 \\cdot 0.99 + 1 \\cdot 0.01 = 1.5445$\n",
    "* Based on these theoretical analyses, and looking at the top plot in Figure 2.2, we see that the $\\epsilon=0.01$ strategy converges slower yet will ultimately converge to a better average reward, whilst the $\\epsilon=0.1$ strategy converges faster yet will ultimately converge to a slightly worse average reward\n",
    "* Both policies noticeably have higher average reward compared to the purely-greedy ($\\epsilon=0$) strategy\n",
    "* Applying the answer to Exercise 2.1, assuming that both $\\epsilon$-greedy policies converge, then each $\\epsilon$-greedy strategy would select the optimal action with a probability of $1.0 \\cdot (1.0-\\epsilon) + 0.1 \\cdot \\epsilon$\n",
    "* Thus, we expect the the $\\epsilon=0.01$ strategy to ultimately converge to a higher optimal action percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
